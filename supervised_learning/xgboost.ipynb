{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7ae76f92-f920-4fae-99b1-5d7a6e4ab99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "94da2efc-735f-4490-8f04-273c28a51c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostTree:\n",
    "    \"\"\"Single tree implementation using XGBoost algorithm\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_depth: int = 6,\n",
    "        min_child_weight: float = 1.0,\n",
    "        gamma: float = 0.0,\n",
    "        lambda_l2: float = 1.0,\n",
    "        min_samples_split: int = 10\n",
    "    ):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_child_weight = min_child_weight\n",
    "        self.gamma = gamma\n",
    "        self.lambda_l2 = lambda_l2\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "\n",
    "    def _calc_leaf_value(self, sum_grad: float, sum_hess: float) -> float:\n",
    "        \"\"\"Calculate leaf value using XGBoost formula\"\"\"\n",
    "        return -sum_grad / (sum_hess + self.lambda_l2)\n",
    "\n",
    "    def _calc_gain(\n",
    "        self,\n",
    "        left_grad: float,\n",
    "        left_hess: float,\n",
    "        right_grad: float,\n",
    "        right_hess: float,\n",
    "        sum_grad: float,\n",
    "        sum_hess: float\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate gain for a split using XGBoost formula\"\"\"\n",
    "        gain = 0\n",
    "        if left_hess >= self.min_child_weight:\n",
    "            gain += (left_grad ** 2) / (left_hess + self.lambda_l2)\n",
    "        if right_hess >= self.min_child_weight:\n",
    "            gain += (right_grad ** 2) / (right_hess + self.lambda_l2)\n",
    "        if sum_hess >= self.min_child_weight:\n",
    "            gain -= (sum_grad ** 2) / (sum_hess + self.lambda_l2)\n",
    "        gain = gain / 2.0 - self.gamma\n",
    "        return gain\n",
    "\n",
    "    def _find_best_split(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        gradients: np.ndarray,\n",
    "        hessians: np.ndarray,\n",
    "        feature_mask: Optional[np.ndarray] = None\n",
    "    ) -> Tuple[Optional[int], Optional[float], Optional[float]]:\n",
    "        \"\"\"Find best split using XGBoost's exact greedy algorithm\"\"\"\n",
    "        best_gain = 0.0\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "\n",
    "        sum_grad = np.sum(gradients)\n",
    "        sum_hess = np.sum(hessians)\n",
    "\n",
    "        if feature_mask is None:\n",
    "            feature_mask = np.ones(X.shape[1], dtype=bool)\n",
    "\n",
    "        for feature in range(X.shape[1]):\n",
    "            if not feature_mask[feature]:\n",
    "                continue\n",
    "\n",
    "            sort_idx = np.argsort(X[:, feature])\n",
    "            sorted_feature = X[sort_idx, feature]\n",
    "            sorted_gradients = gradients[sort_idx]\n",
    "            sorted_hessians = hessians[sort_idx]\n",
    "\n",
    "            left_grad = np.cumsum(sorted_gradients)[:-1]\n",
    "            left_hess = np.cumsum(sorted_hessians)[:-1]\n",
    "            right_grad = sum_grad - left_grad\n",
    "            right_hess = sum_hess - left_hess\n",
    "\n",
    "            unique_values = np.unique(sorted_feature)[:-1]\n",
    "            \n",
    "            for threshold in unique_values:\n",
    "                split_mask = sorted_feature <= threshold\n",
    "                n_left = np.sum(split_mask)\n",
    "                n_right = len(split_mask) - n_left\n",
    "\n",
    "                if n_left < self.min_samples_split or n_right < self.min_samples_split:\n",
    "                    continue\n",
    "\n",
    "                gain = self._calc_gain(\n",
    "                    left_grad[n_left-1],\n",
    "                    left_hess[n_left-1],\n",
    "                    right_grad[n_left-1],\n",
    "                    right_hess[n_left-1],\n",
    "                    sum_grad,\n",
    "                    sum_hess\n",
    "                )\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold, best_gain\n",
    "\n",
    "    def _build_tree(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        gradients: np.ndarray,\n",
    "        hessians: np.ndarray,\n",
    "        depth: int = 0,\n",
    "        feature_mask: Optional[np.ndarray] = None\n",
    "    ) -> Dict:\n",
    "        \"\"\"Recursively build the tree\"\"\"\n",
    "        if depth >= self.max_depth:\n",
    "            return self._calc_leaf_value(np.sum(gradients), np.sum(hessians))\n",
    "\n",
    "        feature, threshold, gain = self._find_best_split(X, gradients, hessians, feature_mask)\n",
    "\n",
    "        if feature is None or gain <= 0:\n",
    "            return self._calc_leaf_value(np.sum(gradients), np.sum(hessians))\n",
    "\n",
    "        left_mask = X[:, feature] <= threshold\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        left_tree = self._build_tree(\n",
    "            X[left_mask],\n",
    "            gradients[left_mask],\n",
    "            hessians[left_mask],\n",
    "            depth + 1,\n",
    "            feature_mask\n",
    "        )\n",
    "        right_tree = self._build_tree(\n",
    "            X[right_mask],\n",
    "            gradients[right_mask],\n",
    "            hessians[right_mask],\n",
    "            depth + 1,\n",
    "            feature_mask\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'feature': feature,\n",
    "            'threshold': threshold,\n",
    "            'left': left_tree,\n",
    "            'right': right_tree,\n",
    "            'gain': gain\n",
    "        }\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        gradients: np.ndarray,\n",
    "        hessians: np.ndarray,\n",
    "        feature_mask: Optional[np.ndarray] = None\n",
    "    ) -> None:\n",
    "        \"\"\"Fit the tree\"\"\"\n",
    "        self.tree = self._build_tree(X, gradients, hessians, feature_mask=feature_mask)\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        return np.array([self._predict_single(x, self.tree) for x in X])\n",
    "\n",
    "    def _predict_single(self, x: np.ndarray, tree: Dict) -> float:\n",
    "        \"\"\"Predict single instance\"\"\"\n",
    "        if not isinstance(tree, dict):\n",
    "            return tree\n",
    "        if x[tree['feature']] <= tree['threshold']:\n",
    "            return self._predict_single(x, tree['left'])\n",
    "        return self._predict_single(x, tree['right'])\n",
    "\n",
    "class XGBoostClassifier:\n",
    "    \"\"\"XGBoost classifier implementation\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_estimators: int = 100,\n",
    "        max_depth: int = 6,\n",
    "        learning_rate: float = 0.3,\n",
    "        min_child_weight: float = 1.0,\n",
    "        gamma: float = 0.0,\n",
    "        lambda_l2: float = 1.0,\n",
    "        colsample_bytree: float = 1.0,\n",
    "        subsample: float = 1.0,\n",
    "        random_state: Optional[int] = None\n",
    "    ):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.min_child_weight = min_child_weight\n",
    "        self.gamma = gamma\n",
    "        self.lambda_l2 = lambda_l2\n",
    "        self.colsample_bytree = colsample_bytree\n",
    "        self.subsample = subsample\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        self.trees: List[List[XGBoostTree]] = []\n",
    "        self.n_classes = None\n",
    "        \n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "\n",
    "    def _compute_gradients_hessians(\n",
    "        self,\n",
    "        y_true: np.ndarray,\n",
    "        y_pred: np.ndarray\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Compute gradients and hessians for multi-class log loss\"\"\"\n",
    "        probas = self._softmax(y_pred)\n",
    "        gradients = probas.copy()\n",
    "        for i in range(len(y_true)):\n",
    "            gradients[i, int(y_true[i])] -= 1\n",
    "        hessians = probas * (1 - probas)\n",
    "        return gradients, hessians\n",
    "\n",
    "    def _softmax(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute softmax probabilities\"\"\"\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> 'XGBoostClassifier':\n",
    "        \"\"\"Fit the XGBoost model\"\"\"\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        n_samples = len(X)\n",
    "\n",
    "        y_pred = np.zeros((n_samples, self.n_classes))\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            gradients, hessians = self._compute_gradients_hessians(y, y_pred)\n",
    "            \n",
    "            n_features = int(X.shape[1] * self.colsample_bytree)\n",
    "            feature_mask = np.random.choice(\n",
    "                [True, False],\n",
    "                size=X.shape[1],\n",
    "                p=[self.colsample_bytree, 1-self.colsample_bytree]\n",
    "            )\n",
    "\n",
    "            sample_mask = np.random.rand(n_samples) < self.subsample\n",
    "            if not np.any(sample_mask):\n",
    "                sample_mask[np.random.randint(n_samples)] = True\n",
    "\n",
    "            trees_this_round = []\n",
    "            \n",
    "            for k in range(self.n_classes):\n",
    "                tree = XGBoostTree(\n",
    "                    max_depth=self.max_depth,\n",
    "                    min_child_weight=self.min_child_weight,\n",
    "                    gamma=self.gamma,\n",
    "                    lambda_l2=self.lambda_l2\n",
    "                )\n",
    "                \n",
    "                tree.fit(\n",
    "                    X[sample_mask],\n",
    "                    gradients[sample_mask, k],\n",
    "                    hessians[sample_mask, k],\n",
    "                    feature_mask\n",
    "                )\n",
    "                \n",
    "                update = tree.predict(X)\n",
    "                y_pred[:, k] += self.learning_rate * update\n",
    "                \n",
    "                trees_this_round.append(tree)\n",
    "            \n",
    "            self.trees.append(trees_this_round)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predict class probabilities\"\"\"\n",
    "        raw_predictions = np.zeros((len(X), self.n_classes))\n",
    "        \n",
    "        for trees_this_round in self.trees:\n",
    "            for k, tree in enumerate(trees_this_round):\n",
    "                raw_predictions[:, k] += self.learning_rate * tree.predict(X)\n",
    "        \n",
    "        return self._softmax(raw_predictions)\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predict classes\"\"\"\n",
    "        probas = self.predict_proba(X)\n",
    "        return np.argmax(probas, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b5200c89-fe25-4a12-9a2b-4cf15ea4fe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationMetrics:\n",
    "    \"\"\"Implementation of classification metrics from scratch\"\"\"\n",
    "    def __init__(self, y_true: np.ndarray, y_pred: np.ndarray):\n",
    "        self.y_true = y_true\n",
    "        self.y_pred = y_pred\n",
    "        self.classes = np.unique(np.concatenate([y_true, y_pred]))\n",
    "        self.n_classes = len(self.classes)\n",
    "        \n",
    "        self.confusion_matrix = self._compute_confusion_matrix()\n",
    "        \n",
    "    def _compute_confusion_matrix(self) -> np.ndarray:\n",
    "        \"\"\"Compute confusion matrix\"\"\"\n",
    "        cm = np.zeros((self.n_classes, self.n_classes), dtype=int)\n",
    "        for i in range(len(self.y_true)):\n",
    "            cm[int(self.y_true[i]), int(self.y_pred[i])] += 1\n",
    "        return cm\n",
    "    \n",
    "    def precision_recall_f1(self) -> dict:\n",
    "        \"\"\"Calculate precision, recall and f1 for each class\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        for class_idx in range(self.n_classes):\n",
    "            tp = self.confusion_matrix[class_idx, class_idx]\n",
    "            fp = np.sum(self.confusion_matrix[:, class_idx]) - tp\n",
    "            fn = np.sum(self.confusion_matrix[class_idx, :]) - tp\n",
    "            \n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            metrics[class_idx] = {\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1-score': f1,\n",
    "                'support': np.sum(self.confusion_matrix[class_idx, :])\n",
    "            }\n",
    "            \n",
    "        return metrics\n",
    "    \n",
    "    def accuracy(self) -> float:\n",
    "        \"\"\"Calculate overall accuracy\"\"\"\n",
    "        return np.sum(np.diag(self.confusion_matrix)) / np.sum(self.confusion_matrix)\n",
    "    \n",
    "    def macro_average(self, metrics: dict) -> dict:\n",
    "        \"\"\"Calculate macro average of metrics\"\"\"\n",
    "        macro_avg = {\n",
    "            'precision': 0.0,\n",
    "            'recall': 0.0,\n",
    "            'f1-score': 0.0,\n",
    "            'support': np.sum([m['support'] for m in metrics.values()])\n",
    "        }\n",
    "        \n",
    "        for m in metrics.values():\n",
    "            macro_avg['precision'] += m['precision']\n",
    "            macro_avg['recall'] += m['recall']\n",
    "            macro_avg['f1-score'] += m['f1-score']\n",
    "        \n",
    "        for key in ['precision', 'recall', 'f1-score']:\n",
    "            macro_avg[key] /= self.n_classes\n",
    "            \n",
    "        return macro_avg\n",
    "    \n",
    "    def weighted_average(self, metrics: dict) -> dict:\n",
    "        \"\"\"Calculate weighted average of metrics\"\"\"\n",
    "        total_support = sum(m['support'] for m in metrics.values())\n",
    "        weighted_avg = {\n",
    "            'precision': 0.0,\n",
    "            'recall': 0.0,\n",
    "            'f1-score': 0.0,\n",
    "            'support': total_support\n",
    "        }\n",
    "        \n",
    "        for class_idx, m in metrics.items():\n",
    "            weight = m['support'] / total_support\n",
    "            weighted_avg['precision'] += m['precision'] * weight\n",
    "            weighted_avg['recall'] += m['recall'] * weight\n",
    "            weighted_avg['f1-score'] += m['f1-score'] * weight\n",
    "            \n",
    "        return weighted_avg\n",
    "    \n",
    "    def generate_report(self) -> str:\n",
    "        \"\"\"Generate classification report similar to sklearn's\"\"\"\n",
    "        metrics = self.precision_recall_f1()\n",
    "        macro_avg = self.macro_average(metrics)\n",
    "        weighted_avg = self.weighted_average(metrics)\n",
    "        \n",
    "        report = \"\\nClassification Report:\\n\"\n",
    "        report += \"              precision    recall  f1-score   support\\n\\n\"\n",
    "        \n",
    "        for class_idx in range(self.n_classes):\n",
    "            m = metrics[class_idx]\n",
    "            report += f\"       class {class_idx:2d}    {m['precision']:8.2f}  {m['recall']:8.2f}  {m['f1-score']:8.2f}  {m['support']:8d}\\n\"\n",
    "        \n",
    "        report += \"\\n\"\n",
    "        report += f\"    macro avg     {macro_avg['precision']:8.2f}  {macro_avg['recall']:8.2f}  {macro_avg['f1-score']:8.2f}  {macro_avg['support']:8d}\\n\"\n",
    "        report += f\" weighted avg     {weighted_avg['precision']:8.2f}  {weighted_avg['recall']:8.2f}  {weighted_avg['f1-score']:8.2f}  {weighted_avg['support']:8d}\\n\"\n",
    "        \n",
    "        total_accuracy = self.accuracy()\n",
    "        report += f\"\\nAccuracy: {total_accuracy:.4f}\\n\"\n",
    "        report += f\"Total samples: {np.sum(self.confusion_matrix)}\\n\"\n",
    "        \n",
    "        return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8d031489-6e70-4c56-9376-1a960ba80afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_balanced_data(n_samples: int = 2000, noise_level: float = 0.4) -> tuple:\n",
    "    \"\"\"Generate moderately complex data with some overlap but clear patterns\"\"\"\n",
    "    n_samples_per_class = n_samples // 4\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    X.append(np.vstack([\n",
    "        np.random.normal([-2, -2], noise_level, (n_samples_per_class//2, 2)),\n",
    "        np.random.normal([-1, -1], noise_level, (n_samples_per_class//2, 2))\n",
    "    ]))\n",
    "    y.append(np.zeros(n_samples_per_class))\n",
    "    \n",
    "    x1 = np.random.uniform(-2, 2, n_samples_per_class)\n",
    "    x2 = x1 + np.random.normal(0, noise_level, n_samples_per_class)\n",
    "    X.append(np.column_stack([x1, x2]))\n",
    "    y.append(np.ones(n_samples_per_class))\n",
    "    \n",
    "    theta = np.random.uniform(0, 2*np.pi, n_samples_per_class)\n",
    "    r = 2 + np.random.normal(0, 0.2, n_samples_per_class)\n",
    "    X.append(np.column_stack([\n",
    "        r * np.cos(theta) + np.random.normal(0, noise_level/2, n_samples_per_class) + 2,\n",
    "        r * np.sin(theta) + np.random.normal(0, noise_level/2, n_samples_per_class) + 2\n",
    "    ]))\n",
    "    y.append(np.full(n_samples_per_class, 2))\n",
    "    \n",
    "    centers = [[-2, 2], [0, 2], [2, 0]]\n",
    "    X_class3 = []\n",
    "    for center in centers:\n",
    "        X_class3.append(\n",
    "            np.random.normal(center, noise_level*0.8, (n_samples_per_class//3, 2))\n",
    "        )\n",
    "    X.append(np.vstack(X_class3))\n",
    "    y.append(np.full(n_samples_per_class, 3))\n",
    "    \n",
    "    X = np.vstack(X)\n",
    "    y = np.hstack(y)\n",
    "    \n",
    "    n_outliers = n_samples // 50\n",
    "    outliers = np.random.uniform(-4, 4, (n_outliers, 2))\n",
    "    outlier_labels = np.random.randint(0, 4, n_outliers)\n",
    "    \n",
    "    X = np.vstack([X, outliers])\n",
    "    y = np.hstack([y, outlier_labels])\n",
    "    \n",
    "    idx = np.random.permutation(len(X))\n",
    "    return X[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4de2158b-eae7-4d7e-8d7a-538a52f3b613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm: np.ndarray, title: str = 'Confusion Matrix') -> None:\n",
    "    \"\"\"Plot confusion matrix using seaborn\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "def plot_results(X_train: np.ndarray, X_test: np.ndarray, y_train: np.ndarray, \n",
    "                y_test: np.ndarray, model: 'XGBoostClassifier') -> None:\n",
    "    \"\"\"Plot the data and model decision boundaries\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    axes[0].scatter(X_train[:, 0], X_train[:, 1], c='gray', alpha=0.5, label='Train')\n",
    "    axes[0].scatter(X_test[:, 0], X_test[:, 1], c='darkgray', alpha=0.5, marker='^', label='Test')\n",
    "    axes[0].set_title('Raw Data')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, 4))\n",
    "    for i in range(4):\n",
    "        mask_train = y_train == i\n",
    "        mask_test = y_test == i\n",
    "        axes[1].scatter(X_train[mask_train, 0], X_train[mask_train, 1], \n",
    "                       c=[colors[i]], alpha=0.5, label=f'Class {i}')\n",
    "        axes[1].scatter(X_test[mask_test, 0], X_test[mask_test, 1], \n",
    "                       c=[colors[i]], alpha=0.5, marker='^')\n",
    "    axes[1].set_title('True Labels')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "    y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                        np.linspace(y_min, y_max, 100))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    axes[2].contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.tab10)\n",
    "    for i in range(4):\n",
    "        mask = y_train == i\n",
    "        axes[2].scatter(X_train[mask, 0], X_train[mask, 1], \n",
    "                       c=[colors[i]], alpha=0.5, label=f'Class {i}')\n",
    "    axes[2].set_title('XGBoost Decision Boundary')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4e793b-63e4-4681-8f50-f6ce490cf0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost model...\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    X, y = generate_balanced_data(n_samples=2000, noise_level=0.4)\n",
    "    train_size = int(0.8 * len(X))\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "    model = XGBoostClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.1,\n",
    "        min_child_weight=1.0,\n",
    "        gamma=0.1,\n",
    "        lambda_l2=1.0,\n",
    "        colsample_bytree=0.8,\n",
    "        subsample=0.8,\n",
    "    )\n",
    "\n",
    "    print(\"Training XGBoost model...\")\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Making predictions...\")\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    train_metrics = ClassificationMetrics(y_train, y_train_pred)\n",
    "    test_metrics = ClassificationMetrics(y_test, y_test_pred)\n",
    "\n",
    "    print(\"\\nTraining Set Performance:\")\n",
    "    print(train_metrics.generate_report())\n",
    "    print(\"\\nTest Set Performance:\")\n",
    "    print(test_metrics.generate_report())\n",
    "\n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    plot_results(X_train, X_test, y_train, y_test, model)\n",
    "    plot_confusion_matrix(test_metrics.confusion_matrix, title='Test Set Confusion Matrix')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
